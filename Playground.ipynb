{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00539cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba0569f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global initialization started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global initialization completed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Global initialization started.\")\n",
    "WORK_DIR = \"/data/disk5/private/yuc/coref/bert-tagger\"\n",
    "FILE_LIST = \"filelist.txt\"\n",
    "WIKI_DIR = os.path.join(WORK_DIR, \"../wikipedia/text\")\n",
    "# DUMP_DIR =  os.path.join(WORK_DIR, \"playground/dump\")\n",
    "DUMP_DIR = os.path.join(WORK_DIR, \"playground/dump_kl_para\")\n",
    "LOG_DIR = os.path.join(WORK_DIR, \"playground/logs\")\n",
    "\n",
    "global_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "global_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "print(\"Global initialization completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "71a28225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': tensor([ 101, 2000, 2022, 2030, 2025, 2000, 2022, 1010, 2023, 2003, 1996, 3160,\n",
      "        1012,  102]), 'raw': ['to', 'be', 'or', 'not', 'to', 'be', ',', 'this', 'is', 'the', 'question', '.']}\n"
     ]
    }
   ],
   "source": [
    "def default_transform(sentence):\n",
    "    if sentence[0] == \"<\":\n",
    "        return None\n",
    "    # raw_tokens: parsed into subword but not yet converted to ids\n",
    "    raw_tokens = global_tokenizer.tokenize(sentence.strip())\n",
    "    # tokens converted to ids\n",
    "    tokens = global_tokenizer(raw_tokens, return_tensors=\"pt\", is_split_into_words=True)[\"input_ids\"]\n",
    "    tokens = torch.squeeze(tokens)\n",
    "    # l = len(tokens)\n",
    "    l = tokens.shape[0]\n",
    "    if l >= 30: # ignore doc that is too long\n",
    "        return None\n",
    "    if l <= 5: # ignore invalid lines and short sentences\n",
    "        return None\n",
    "\n",
    "    # this is the format of a sentence.\n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"raw\": raw_tokens\n",
    "    }\n",
    "\n",
    "ts = default_transform(\"To be or not to be, this is the question.\")\n",
    "print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4d6147f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceIterable:\n",
    "    def __init__(self,\n",
    "        file_path_list=FILE_LIST,\n",
    "        file_id=0,\n",
    "        stc_id=0,\n",
    "        transform=default_transform):\n",
    "        self.file_id = file_id\n",
    "        self.stc_id = stc_id\n",
    "        with open(file_path_list, \"r\") as f_list:\n",
    "            self.file_paths = f_list.read().split()\n",
    "        if transform == None:\n",
    "            self.transform = default_transform\n",
    "        else:\n",
    "            self.transform = transform\n",
    "        print(\"SentenceIterable constructed.\")\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self.sentence_generator()\n",
    "    \n",
    "    def sentence_generator(self):\n",
    "        file_count = len(self.file_paths)\n",
    "        while self.file_id < file_count:\n",
    "            file_path = self.file_paths[self.file_id]\n",
    "            with open(file_path) as fs:\n",
    "                sentences = fs.readlines()\n",
    "                sentence_count = len(sentences)\n",
    "                while self.stc_id < sentence_count:\n",
    "                    sentence = sentences[self.stc_id]\n",
    "                    sentence = self.transform(sentence)\n",
    "                    if sentence == None:\n",
    "                        print(\"sentence discarded.\")\n",
    "                    else:\n",
    "                        yield (sentence, self.file_id, self.stc_id)\n",
    "                    self.stc_id += 1\n",
    "            self.stc_id = 0\n",
    "            self.file_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0edd5fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionPairIterable constructed.\n",
      "{'label': tensor([1, 2, 3, 4, 5]), 'unmasked': tensor([  1, 103,   3,   4,   5]), 'masked': tensor([  1, 103, 103,   4,   5]), 'miss_id': tensor([1]), 'mask_id': tensor([2])}\n",
      "{'label': tensor([1, 2, 3, 4, 5]), 'unmasked': tensor([  1, 103,   3,   4,   5]), 'masked': tensor([  1, 103,   3, 103,   5]), 'miss_id': tensor([1]), 'mask_id': tensor([3])}\n",
      "{'label': tensor([1, 2, 3, 4, 5]), 'unmasked': tensor([  1,   2, 103,   4,   5]), 'masked': tensor([  1, 103, 103,   4,   5]), 'miss_id': tensor([2]), 'mask_id': tensor([1])}\n",
      "{'label': tensor([1, 2, 3, 4, 5]), 'unmasked': tensor([  1,   2, 103,   4,   5]), 'masked': tensor([  1,   2, 103, 103,   5]), 'miss_id': tensor([2]), 'mask_id': tensor([3])}\n",
      "{'label': tensor([1, 2, 3, 4, 5]), 'unmasked': tensor([  1,   2,   3, 103,   5]), 'masked': tensor([  1, 103,   3, 103,   5]), 'miss_id': tensor([3]), 'mask_id': tensor([1])}\n",
      "{'label': tensor([1, 2, 3, 4, 5]), 'unmasked': tensor([  1,   2,   3, 103,   5]), 'masked': tensor([  1,   2, 103, 103,   5]), 'miss_id': tensor([3]), 'mask_id': tensor([2])}\n"
     ]
    }
   ],
   "source": [
    "class QuestionPairIterable(Dataset):\n",
    "    def __init__(self, \n",
    "        sentence,\n",
    "        mask_placeholder=\"[MASK]\",\n",
    "        miss_placeholder=\"[MASK]\"):\n",
    "        super(QuestionPairIterable).__init__()\n",
    "        self.sentence = sentence[\"tokens\"]\n",
    "        self.miss_ph = miss_placeholder\n",
    "        self.mask_ph = mask_placeholder\n",
    "        self.miss_id = global_tokenizer.convert_tokens_to_ids(miss_placeholder)\n",
    "        self.mask_id = global_tokenizer.convert_tokens_to_ids(mask_placeholder)\n",
    "        length = len(self.sentence)\n",
    "        self.index_pairs = [\n",
    "            ([miss_index], [mask_index])\n",
    "            for miss_index in range(1, length-1)\n",
    "                for mask_index in range(1, length-1)\n",
    "                    if miss_index != mask_index\n",
    "        ]\n",
    "\n",
    "        self.start = 0\n",
    "        self.end = len(self.index_pairs)\n",
    "        print(\"QuestionPairIterable constructed.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        missing_indices, masked_indices = self.index_pairs[index]\n",
    "        # unmasked_question = list(self.sentence)\n",
    "        unmasked_question = self.sentence.clone()\n",
    "        for missing_index in missing_indices:\n",
    "            # unmasked_question[missing_index] = self.miss_ph\n",
    "            unmasked_question[missing_index] = self.miss_id\n",
    "        # masked_question = list(unmasked_question)\n",
    "        masked_question = unmasked_question.clone()\n",
    "        for masked_index in masked_indices:\n",
    "            # masked_question[masked_index] = self.mask_ph\n",
    "            masked_question[masked_index] = self.mask_id\n",
    "        return {\n",
    "            \"label\": self.sentence,\n",
    "            \"unmasked\": unmasked_question, \n",
    "            \"masked\": masked_question, \n",
    "            \"miss_id\": torch.tensor(missing_indices), \n",
    "            \"mask_id\": torch.tensor(masked_indices)\n",
    "        }\n",
    "    \n",
    "def test_pair_iterable():\n",
    "    sentence = {\n",
    "        \"tokens\": torch.from_numpy(np.array([1,2,3,4,5]))\n",
    "    }\n",
    "    dataset = QuestionPairIterable(sentence)\n",
    "    for sample in dataset:\n",
    "        print(sample)\n",
    "\n",
    "test_pair_iterable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "34833ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionPairConsumer:\n",
    "    def __init__(self,\n",
    "        tokenizer=global_tokenizer,\n",
    "        model=global_model,\n",
    "        measure=kl_divergence_dist):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.measure = measure\n",
    "    \n",
    "    def consume_question_pair(self, question_pair):\n",
    "        # [B(atch), L(ength of sentence)]\n",
    "        context = question_pair[\"label\"]\n",
    "        unmasked = question_pair[\"unmasked\"]\n",
    "        masked = question_pair[\"masked\"]\n",
    "        # [B(atch), n(umber of missing tokens)]\n",
    "        missing_indices = question_pair[\"miss_id\"]\n",
    "        masked_indices = question_pair[\"mask_id\"]\n",
    "        # u_pred = consume_question(unmasked, context)\n",
    "        # m_pred = consume_question(masked, unmasked)\n",
    "        # [B(atch), L(ength of sentence), V(ocabulary size)]\n",
    "        u_logits = self.model(input_ids=unmasked).logits\n",
    "        m_logits = self.model(input_ids=masked).logits\n",
    "\n",
    "        missing_label_ids = torch.gather(context, 1, missing_indices) # [B, n]\n",
    "        answer_shape = list(missing_indices.shape)\n",
    "        answer_shape.append(u_logits.shape[2])\n",
    "        missing_indices = missing_indices.unsqueeze(2).expand(answer_shape) # [B, n, V]\n",
    "        missing_label_ids = missing_label_ids.unsqueeze(2).expand(answer_shape) # [B, n, V]\n",
    "        \n",
    "        ones_template = torch.tensor([[[1.]]]).expand(answer_shape) # [B, n, V]\n",
    "        # golden logits ,g_logits[b][n][index[b][n]] = 1\n",
    "        g_logits = torch.scatter(torch.zeros(answer_shape), 2, missing_label_ids, ones_template)\n",
    "        # unmasked logits\n",
    "        u_logits = torch.gather(u_logits, 1, missing_indices)\n",
    "        # masked logits\n",
    "        m_logits = torch.gather(m_logits, 1, missing_indices)\n",
    "        \n",
    "        return self.measure(m_logits, u_logits, g_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5b940e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_size(batch):\n",
    "    for value in batch.values():\n",
    "        return value.shape[0]\n",
    "\n",
    "class SaveManager:\n",
    "    def __init__(self,\n",
    "        dump_dir=DUMP_DIR,\n",
    "        counter=0,\n",
    "        log_interval=100,\n",
    "        save_interval=500):\n",
    "        self.sentence_dict = {}\n",
    "        self.relation_list = []\n",
    "        self.log_interval = log_interval\n",
    "        self.save_interval = save_interval\n",
    "        self.counter = counter - counter % save_interval\n",
    "        self.dump_dir = dump_dir\n",
    "        self.progress_path = os.path.join(self.dump_dir, \"progress.log\")\n",
    "        self.rel_template = os.path.join(dump_dir, \"relation_list_cnt_{}.dump\")\n",
    "        self.stc_template = os.path.join(dump_dir, \"sentence_dict_cnt_{}.dump\")\n",
    "\n",
    "    def load_progress(self):\n",
    "        return (0, 0)\n",
    "\n",
    "        if os.path.exists(self.progress_path):\n",
    "            with open(self.progress_path, \"r\") as p_log:\n",
    "                progress = json.load(p_log)\n",
    "                file_id = progress[\"file_id\"]\n",
    "                stc_id = progress[\"stc_id\"]\n",
    "                self.save_interval = progress[\"save_interval\"]\n",
    "                self.counter = progress[\"counter\"]\n",
    "                return (file_id, stc_id)\n",
    "        return (0, 0)\n",
    "\n",
    "    def dump_progress(self, file_id, stc_id):\n",
    "        with open(self.progress_path, \"w\") as p_log:\n",
    "            progress = {\n",
    "                \"file_id\": file_id,\n",
    "                \"stc_id\": stc_id,\n",
    "                \"counter\": self.counter,\n",
    "                \"save_interval\": self.save_interval\n",
    "            }\n",
    "            p_log.write(json.dumps(progress))\n",
    "        \n",
    "    def save_sentence_list(self):\n",
    "        sentence_list = []\n",
    "        for context_id, raw_tokens in self.sentence_dict:\n",
    "            self.sentence_list.append({\n",
    "            \"id\": context_id,\n",
    "            \"context\": raw_tokens\n",
    "        })\n",
    "        sentence_list.sort(key=lambda x:x[\"id\"])\n",
    "        save_path = self.stc_template.format(self.counter)\n",
    "        with open(save_path, \"w\") as f:\n",
    "            for sentence in sentence_list:\n",
    "                f.write(json.dumps(sentence)+\"\\n\")\n",
    "\n",
    "    def update_sentence(self, sentence, context_id):\n",
    "        self.sentence_dict[context_id] = sentence[\"raw\"]\n",
    "    \n",
    "    def save_relation_list(self):\n",
    "        save_path = self.rel_template.format(self.counter)\n",
    "        with open(save_path, \"w\") as f:\n",
    "            for relation in self.relation_list:\n",
    "                f.write(json.dumps(relation)+\"\\n\")\n",
    "    \n",
    "    def update_relation(self, sample, distance, context_id):\n",
    "        self.relation_list.append({\n",
    "            \"context\": context_id,\n",
    "            \"missing_index\": sample[\"miss_id\"].tolist(),\n",
    "            \"masked_index\": sample[\"mask_id\"].tolist(),\n",
    "            \"distance\": float(distance)\n",
    "        })\n",
    "        self.counter += 1\n",
    "        if self.counter % self.log_interval == 0:\n",
    "            print(\"Got example count: \", self.counter)\n",
    "        if self.counter % self.save_interval == 0:\n",
    "            print(\"Save examples.\")\n",
    "            save_relation_list()\n",
    "            save_sentence_list()\n",
    "            self.relation_list = []\n",
    "            self.sentence_dict = {}\n",
    "\n",
    "    def update_relation_batched(self, batch, distance, context_id):\n",
    "        batch_size = get_batch_size(batch)\n",
    "        for index in range(0, batch_size):\n",
    "            relation = {}\n",
    "            for key, batched_tensor in batch.items():\n",
    "                relation[key] = batched_tensor[index]\n",
    "            self.update_relation(relation, distance[index], context_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "bff0f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    sentence_list = []\n",
    "    relation_list = []\n",
    "    log_interval = 100\n",
    "    save_interval = 500\n",
    "\n",
    "    save_manager = SaveManager(save_interval=save_interval)\n",
    "    last_file_id, last_stc_id = save_manager.load_progress()\n",
    "   \n",
    "    sentence_dataset = SentenceIterable(file_id=last_file_id,stc_id=last_stc_id)\n",
    "    consumer = QuestionPairConsumer() \n",
    "\n",
    "    for sentence, file_id, stc_id in sentence_dataset:\n",
    "        context_id = file_id * 50000 + stc_id\n",
    "        stc_relation_list = []\n",
    "        question_pair_dataset = QuestionPairIterable(sentence)\n",
    "        dataloader = DataLoader(question_pair_dataset, batch_size=32, num_workers=0)\n",
    "        for sample_batched in dataloader:\n",
    "            distance = consumer.consume_question_pair(sample_batched)\n",
    "            save_manager.update_sentence(sentence, context_id)\n",
    "            save_manager.update_relation_batched(sample_batched, distance, context_id)\n",
    "            save_manager.dump_progress(file_id, stc_id)\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "66b39e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceIterable constructed.\n",
      "sentence discarded.\n",
      "QuestionPairIterable constructed.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "daaab200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n",
      "SentenceIterable constructed.\n",
      "sentence discarded.\n",
      "10\n",
      "QuestionPairIterable constructed.\n",
      "{'label': tensor([[  101, 28506,  1516,  2849,  6590,  3351,  3231,  2005,  9874,   102],\n",
      "        [  101, 28506,  1516,  2849,  6590,  3351,  3231,  2005,  9874,   102],\n",
      "        [  101, 28506,  1516,  2849,  6590,  3351,  3231,  2005,  9874,   102],\n",
      "        [  101, 28506,  1516,  2849,  6590,  3351,  3231,  2005,  9874,   102]]), 'unmasked': tensor([[ 101,  103, 1516, 2849, 6590, 3351, 3231, 2005, 9874,  102],\n",
      "        [ 101,  103, 1516, 2849, 6590, 3351, 3231, 2005, 9874,  102],\n",
      "        [ 101,  103, 1516, 2849, 6590, 3351, 3231, 2005, 9874,  102],\n",
      "        [ 101,  103, 1516, 2849, 6590, 3351, 3231, 2005, 9874,  102]]), 'masked': tensor([[ 101,  103,  103, 2849, 6590, 3351, 3231, 2005, 9874,  102],\n",
      "        [ 101,  103, 1516,  103, 6590, 3351, 3231, 2005, 9874,  102],\n",
      "        [ 101,  103, 1516, 2849,  103, 3351, 3231, 2005, 9874,  102],\n",
      "        [ 101,  103, 1516, 2849, 6590,  103, 3231, 2005, 9874,  102]]), 'miss_id': tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]]), 'mask_id': tensor([[2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5]])}\n"
     ]
    }
   ],
   "source": [
    "sample = main()\n",
    "label = sample[\"label\"]\n",
    "unmasked = sample[\"unmasked\"]\n",
    "masked = sample[\"masked\"]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a3a0771c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 1, 30522]\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]]) tensor([[[-6.9024, -6.7771, -6.7575,  ..., -7.0699, -6.2059, -6.7337]],\n",
      "\n",
      "        [[-6.9024, -6.7771, -6.7575,  ..., -7.0699, -6.2059, -6.7337]],\n",
      "\n",
      "        [[-6.9024, -6.7771, -6.7575,  ..., -7.0699, -6.2059, -6.7337]],\n",
      "\n",
      "        [[-6.9024, -6.7771, -6.7575,  ..., -7.0699, -6.2059, -6.7337]]],\n",
      "       grad_fn=<GatherBackward>) tensor([[[-6.9170, -6.7934, -6.7733,  ..., -7.1199, -6.1876, -6.8410]],\n",
      "\n",
      "        [[-6.9155, -6.7748, -6.8467,  ..., -6.9335, -6.4507, -6.2258]],\n",
      "\n",
      "        [[-6.6585, -6.5278, -6.6587,  ..., -6.7598, -6.3038, -6.3652]],\n",
      "\n",
      "        [[-6.6641, -6.6089, -6.6145,  ..., -7.1140, -6.7535, -5.7576]]],\n",
      "       grad_fn=<GatherBackward>)\n"
     ]
    }
   ],
   "source": [
    "consumer = QuestionPairConsumer()\n",
    "g_logits, u_logits, m_logits = consumer.consume_question_pair(sample)\n",
    "print(g_logits, u_logits, m_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "293bbd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0502, 0.0000, 0.0000, 0.2214], grad_fn=<MeanBackward1>)\n",
      "tensor([0.0186, 0.2816, 0.3189, 0.4473], grad_fn=<MeanBackward1>)\n",
      "tensor([5.5009, 5.9601, 7.2967, 7.6955], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# result: masked; target: unmasked index: golden\n",
    "def index_only_dist(result, target, index):\n",
    "    n_dim = 1\n",
    "    v_dim = 2\n",
    "    return torch.mean(\n",
    "        torch.sum(\n",
    "            F.relu(target - result) * index, dim=v_dim\n",
    "        ),\n",
    "        dim=n_dim\n",
    "    )\n",
    "\n",
    "def kl_divergence_dist(result, target, index):\n",
    "    n_dim = 1\n",
    "    v_dim = 2\n",
    "    return torch.mean(\n",
    "        torch.sum(\n",
    "            F.softmax(target, dim=v_dim) * ( - F.log_softmax(result, dim=v_dim) + \n",
    "            F.log_softmax(target, dim=v_dim)),\n",
    "            dim=v_dim\n",
    "        ),\n",
    "        dim=n_dim\n",
    "    )\n",
    "\n",
    "def cross_entropy_dist(result, target, index):\n",
    "    n_dim = 1\n",
    "    v_dim = 2\n",
    "    return torch.mean(\n",
    "        torch.sum(\n",
    "            - F.softmax(target, dim=v_dim) * F.log_softmax(result, dim=v_dim),\n",
    "            dim=v_dim\n",
    "        ),\n",
    "        dim=n_dim\n",
    "    )\n",
    "\n",
    "print(index_only_dist(u_logits, m_logits, g_logits))\n",
    "print(kl_divergence_dist(u_logits, m_logits, g_logits))\n",
    "print(cross_entropy_dist(u_logits, m_logits, g_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6879223c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n",
      "tensor([1, 2, 3, 4])\n",
      "tensor([1, 2, 3, 4])\n",
      "tensor([1, 2, 3, 4])\n",
      "tensor([1, 2, 3, 4])\n",
      "tensor([1, 2, 3, 4])\n",
      "tensor([1, 2, 3, 4])\n",
      "tensor([1, 2, 3, 4])\n",
      "tensor([1, 2, 3, 4])\n",
      "tensor([1, 2, 3, 4])\n",
      "tensor([1, 2, 3, 4])\n",
      "tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # self.data = [\"To be or not to be, this is the question.\".split(),] * 12\n",
    "        # self.data = [[1,2,3,4],] * 12\n",
    "        self.data = [torch.tensor([1,2,3,4]),] * 12\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "dataset = MyDataset()\n",
    "for sample in dataset:\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8ef3dd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [1, 2, 3, 4],\n",
      "        [1, 2, 3, 4],\n",
      "        [1, 2, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "def test_dataloader():\n",
    "    dataset = MyDataset()\n",
    "    dataloader = DataLoader(dataset, batch_size = 4)\n",
    "    for sample in dataloader:\n",
    "        print(sample)\n",
    "        break\n",
    "\n",
    "test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9f75b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e86500e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2])\n",
      "tensor([0, 3, 4])\n",
      "[[0, 1, 2], [0, 3, 4]]\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[0,1,2],[0,3,4]])\n",
    "for ele in a:\n",
    "    print(ele)\n",
    "b = a.tolist()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4e7949fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values([2, 4])\n",
      "{1: 2, 3: 4, 5: 6}\n"
     ]
    }
   ],
   "source": [
    "a = {1:2,3:4}\n",
    "print(a.values())\n",
    "def test(d):\n",
    "    d.update({5:6})\n",
    "test(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50882aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
